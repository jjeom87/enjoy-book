# 구글 엔지니어에게 듣는 네트워킹과 웹 성능 최적화 기법

## 1부 네트워킹 기초
### 1장 레이턴시와 대역폭 이해의 첫걸음
#### 속도는 하나의 특징이다
* 레이턴시 : 패킷을 전송하는 곳에서부터 전달받는 곳까지 이동하는 데 걸리는 시간
* 대역폭 : 논리적인 혹은 물리적인 통신 경로의 최대 처리량
#### 레이턴시의 구성요소
* 전파지연 (propagation delay) : 메시지가 송신측(Sender)에서 수신측(Receiver)으로 이동하는데 필요한 시간. 총 이동 거리 대비 신호가 이동하는 속도로 측정된다.
* 전송지연 (transmission delay) : 링크롤 패킷을 모든 비트를 내보내는 데 필요한 시간. 패킷의 길이 대비 링크의 데이터 전송 속도로 측정된다.
* 프로세싱 지연 (processing delay) : 패킷 해더를 처리하고 비트 수준의 에러를 체크하고 패킷의 목적지를 알아내는 데 필요한 시간.
* 큐잉 지연 (queuing delay) : 패킷이 처리될 때까지 큐(queue)에서 대기하는 시간
#### 빛의 속도와 전파 지연
* 광섬유의 보편적인 굴절률은 1.4 ~ 1.6
* 사용자들을 잘 붙잡아 두는 동시에 최고의 사용자 경험을 제공하기 위해서 애플리케이션은 수백 밀리초 이내에 응답할 수 있어야 함
#### 최종 마일 레이턴시
* 최종 마일 레이턴시는 인터넷 제공업체, 사용되는 기술, 네트워크의 토폴로지, 시간대에 따라 변경됨
#### 코어 네트워크의 대역폭
* 광섬유는 파장 분할 다중화를 통해 각 섬유마다 다른 파장의 빛을 이동시킬 수 있으므로 대역폭에 있어서는 확실한 이점을 가짐
* 광섬유 링크의 총 대역폭은 다중 송신되는 채널의 수에 채널별 데이터 전송률을 곱한 만큼의 값
#### 네트워크 가장자리에서의 대역폭
* 접속자 수가 많거나 하드웨어 오류 혹은 네트워크를 향한 디도스(DDOS) 공격 같은 여러가지 원인으로 인해 네트워크가 정체될 수 있음
#### 높은 대역폭과 낮은 레이턴시 제공
* 대역폭과 빛의 속도라는 한계점을 분명하게 인식하여 프로토콜과 네트워킹 코드를 설계하고 최적화해야만 애플리케이션 성능을 향상 시킬 수 있음
### 2장 TCP의 구성요소
* IP(Internet Protocol)는 호스트에서 호스트로의 라우팅과 주소할당 기능을 제공
* TCP(Transmission Control Protocol) 신뢰성을 보장해주는 네트워크 기능을 제공
* TCP는 신뢰할 수 없는 채널 위에 신뢰성을 구축한 추상 계층
* 네트워크 통신 구현에 필요한 유실 데이터 재전송, 전송 순서 확인, 혼잡 제어 및 회피 데이터 무결성 확인 같은 복잡한 기능을 투명하게 처리하여 애플리케이션 구현을 한결 쉽게 만듬
* TCP 스트림의 특징 중 하나가, 전송된 모든 바이트는 수신된 모든 바이트와 한 치의 오차 없이 동일
* 클라이언트에서 전송한 바이트 순서대로 도착
* TCP는 신속한 데이터 전송보다는 정확한 데이터의 전송에 더 특화
* HTTP 표준은 전송 프로토콜로서 오직 TCP만을 명시하고 있지 않음
#### 3-Way 핸드셰이크
* SYN : 클라이언트가 무작위로 시퀀스 번호 X를 고르고 SYN 패킷을 보낸다. 그 밖의 다른 TCP 플래그나 옵션 값들을 포함할 수 있다.
* SYN ACK : 서버가 시퀀스 번호 X를 1만큼 증가시키고, 무작위 시퀀스 번호 y를 고른다. 서버 또한 플래그와 옵션 값들을 추가한 후 응답을 보낸다.
* ACK : 클라이언트가 X와 Y를 모두 1만큼 증가시킨 후 마지막 ACK 패킷을 보냄으로써 핸드셰이크 과정을 종료한다.
* 3-Way 핸드셰이크가 끝나면 애플리케이션 데이터가 클라이언트와 서버 사이에 오고 갈 수 있음
* 3-Way 핸드셰이크로 인해 발생하는 지연이 크기 때문에, 새로운 TCP 커넥션을 맺는 것보다 기존에 연결되어 있는 TCP 커넥션을 재사용하는 것이 TCP에서 작동하는 애플리케이션을 최적화하는 데에 아주 중요한 역할을 함
#### 혼잡 제어 및 회피
##### 흐름제어
* 흐름 제어는 송신자가 수신자에게 처리하지 못할 만큼의 많은 데이터를 전송하는 것을 미리 방지하는 매커니즘
* 리시브 윈도를 통지하여 수신 데이터를 저장할 버퍼 공간의 크기를 서로에게 알려줌
* 일단 커넥션이 이루어지면 양쪽에서 자신들의 시스템 기본 설정값을 이용하여 rwnd 값을 초기화함
##### 느린 시작
* 혼잡 윈도 크기 : 클라이언트로부터 응답 확인 신호를 받기 전에 송신자 측에서 지정하는 최대 송신 데이터량
* 서버와 클라이언트는 어떻게 혼잡 윈도 크기의 최적값을 알아내는 것일까?
* 커넥션의 초반에는 천천히 시작해서 ACK 패킷을 받으면서 점점 윈도 사이즈를 늘려 나가는 것
* 느린 시작이 최대로 사용할 수 있는 대역폭에 제한을 둠으로써 용량이 작은 데이터를 전송하는 데에는 부작용으로 다가옴
##### 혼잡 회피
* 패킷 손실이 일어나지 않는 경우는 없고 그보다 패킷 손실이 언제 발생하는가를 파악하는 것이 더 중요
* 암묵적으로 판단하기에 패킷 손실이 일어났다는 것은 네트워크 혼잡이 일어났다는 신호
* 이동 경로의 어딘가에서 정체가 일어난 링크나 라우터가 패킷을 손실을 막기 위해 원도 사이즈를 조정
* 일단 혼잡 윈도가 리셋되면, 혼잡 회피는 더 이상의 손실을 최소화 하기 위해 얼마나 윈도 크기를 늘려야 할지를 지정
#### 대역폭 지연 곱
* 데이터 링크의 허용량과 종단 간 지연을 곱한 값, 결과값은 ACK를 받지 않고 이동할 수 있는 데이터의 최대 양
#### Head-of-Line 블로킹
* 수신자에게 이동 중인 패킷 하나가 소실되면 다른 모든 패킷들은 소실된 패킷이 재전송될 때까지 수신자 쪽 TCP 버퍼에서 대기
* TCP 계층 내에서 이루어지기 때문에 애플리케이션에서는 TCP의 데이터 재전송 여부나 큐에 들어있는 패킷 버퍼를 살펴볼 수가 없음
* 애플리케이션 입장에서는 그저 소켓에서 데이터를 읽으려 했을 때 전달 지연을 겪게 될 뿐
* HOL 블로킹의 
  * 장점
    * 애플리케이션이 패킷의 재배치나 재조합에 관여할 필요가 없기 때문에 애플리케이션의 코드 자체는 훨씬 간단해짐
  * 단점
    * 패킷이 도착하는 시간이 들쭉날쭉 (jitter) 해서 레이턴시를 예측하기 어렵다는 점
#### TCP의 최적화
##### 서버 설정 조정하기
* 사용하는 호스트를 최신 시스템 버전으로 업그레이드
* TCP의 초기 혼잡 원도 크기 증가
* 느린 시작 다시 시작하기
* 윈도 스케일링
* TCP Fast Open
##### 애플리케이션의 동작 튜닝하기
* 비트를 보내지 않는 것보다 빠른 방법은 없다. 즉, 더 적은 수의 비트를 전송
* 데이터를 빨리 이동하게 할 수는 없지만 이동 거리를 줄일 수는 있다.
* TCP 커넥션 재사용은 성능 향상에 매우 중요하다.
##### 성능 체크리스트
* 서버 커널을 최신버전으로 업그레이드하라.
* 혼잡 윈도 크기를 10으로 설정하라.
* 유휴 상태 후 느린 시작을 비활성화하라.
* 윈도 스케일링을 활성화하라.
* 전송 데이터를 압축하라.
* 서버를 사용자와 가까운 곳에 배치하여 왕복 시간을 줄여라.
* 기존 TCP 거넥션을 가능한 한 재사용하라.
### 3장 UDP의 구성요소
* 데이터그램 : 전송 네트워크 계층에서 보장하는 신뢰 기반의 데이터 교환에 의존하지 않고, 충분한 양의 정보를 발신지에서 목적지까지 스스로 운반할 수 있는 독립적인 데이터 개체.
#### Null 프로토콜 서비스
* 메시지를 무사히 운반할 수 있다는 보장 없음
* 메시지를 순서대로 운반할 수 없음
* 커넥션 상태 트래킹 없음
* 혼잡 제어 없음
#### UDP와 네트워크 주소 변환기
* IPv4 주소 부족 문제를 해결하기 위해 1994년 중반에 IP Network Address Translator (RFC 1631) 스펙이 처음으로 소개
* 네트워크 끝에 NAT 기기를 설치해서, 로컬 IP와 포트 번호를 한 개 이상의 고유 공용 IP와 포트 번호에 짝지어 관리
##### 연결 상태 타임아웃
* UDP에서 NAT 변환 작업을 할 때 가장 큰 문제는 데이터 운반을 위해 라우팅 레이블을 관리
* UDP는 커넥션 상태 정보가 없으므로 참조할 수 있는 것이 없음
##### NAT 통과
* NAT 안쪽에 위치한 내부 클라이언트가 자신의 공용 IP를 모름
* 공용 IP를 안다고 해서 무조건 UDP로 데이터를 전송할 수 있는 것은 아님
* NAT의 공용 IP에 도착하는 모든 패킷은 반드시 목적지 포트를 포함해야 하고, 그 값이 NAT 테이블 내에 있어야 하며, 이를 내부 수신 호스트 IP와 포트 튜플로 변환할 수 있어야 함
##### STUN, TURN 그리고 ICE
* STUN (Session Traversal Utilities for NAT) : 호스트 애플리케이션이 네트워크상의 NAT 기기를 발견하고 현재의 커넥션에 지정된 공용 IP와 포트 튜플을 알아낼 수 있게 하는 프로토콜
* TURN (Traversal Using Relays around NAT) : STUN 실패했을 때 UDP를 버리고 TCP로 전환하는 기능\
* ICE (Interactive Connectivity Establishment) : 네트워크 참여자 간에 가장 효율적인 터널을 찾을 수 있는 프로토콜
#### UDP 최적화
* 인터넷 경로 상황에 폭넑게 대응할 수 있어야 한다.
* 데이터 전송률을 조절할 수 있어야 한다.
* 모든 트래픽의 혼잡 제어를 수행할 수 있어야 한다.
* TCP와 비슷한 대역폭을 사용해야 한다.
* 패킷 손실이 있을 때에는 재전송 카운터를 중단해야 한다.
* Path MTU를 넘어서는 데이터그램을 전송해서는 안 된다.
* 데이터그램 손실, 중복, 재정렬을 처리할 수 있어야 한다.
* 최대 전송지연 2분까지 감당할 수 있도록 설계해야 한다.
* IPv4 UDP 체크섬과 IPv6 체크섬을 반드시 활성화해야 한다.
* 필요한 경우 최소 15초 간격으로 킵얼라이브를 사용할 수 있다.
### 4장 전송 계층 보안
* SSL 프로토콜은 넷스케이프에서 전자상거래 보안을 강화하기 위해 처음 개발됨
* 암호화, 인증처리, 데이터무결성 등 여러 보안 기제가 필요한데, SSL 프로토콜은 TCP의 바로 위애서 작동함 (세션계층 ,TLS)
* SSL의 명칭이 TLS(Transport Layer Security, 전송 계층 보안)로 변경 
#### 암호화, 인증 그리고 무결성
* 암호화 : 한 컴퓨터에서 다른 컴퓨터로 보내는 데이터를 타인이 알아볼 수 없도록 하는 매커니즘
* 인증 : 제공된 식별 정보의 진위 여부를 확인하는 메커니즘
* 무결성 : 메시지가 무단으로 변경되었거나 위조되었는지 확인하는 메커니즘
#### TLS 핸드셰이크
* 클라이언트와 서버가 TLS를 통해 애플리케이션 데이터를 주고받기 전에, 먼저 암호화된 터널을 형성해야 함
* 모든 TLS 커넥션이 TCP 핸드셰이크에 추가로 최대 두 번의 왕복 시간을 요구함
##### 애플리케이션 계층 프로토콜 협상(ALPN) ??
* TLS 핸드셰이크 과정에서 애플리케이션 프로토콜 협상할 수 있게 하는 TLS 확장
##### 서버 이름 표시 (Server Name Indication, SNI)
* 서버 이름 표시는 TLS 핸트셰이크가 시작될 때 클라이언트가 연결하려는 호스트 명을 지정 할 수 있다. 
따라서 웹 서버가 서버 이름 표시 호스트 명을 살펴보고 해당 인증서를 선택하여 핸트세이크를 진행 할 수 있음
#### TLS 세션 재개
* TSL는 여러 커넥션 사이에서 동일한 비밀키 데이터를 재개하거나 공유할 수 있음
##### 세션 식별자 (ID)
* 서버가 TLS 협상 도중에 "ServerHello" 메세지의 일부로 32바이트 세션 식별자를 생성하고 전송하도록 하는 장치
##### 세션 티켓
* 세션 티켓 교체 메커니즘을 사용하면 서버가 각 클라이언트마다 고유 세션 상태를 보유하지 않음
* 클라이언트가 서버에게 세션 티켓 지원 여부를 알려주면, TLS 핸드셰이크의 마지막 교환 단계에서 서버가 자신만의 비밀키로 모든 세션 데이터를 암호화하여 세션 티켓 레코드를 포함
* 세션 티켓은 클라이언트가 보관
#### 신뢰 사슬(Chain of Trust)과 인증기관
* 수동으로 지정된 인증서 : 모든 브라우저와 운영체제에는 사용자가 신뢰하는 인증서를 수동으로 불러올 수 있는 기능이 있다. 그 인증서를 어떤 경로로 얻어서 신뢰성을 확인할지는 모두 사용자에게 달려 있다.
* 인증기관 : 인증기관(CA)은 인증서의 사용자와 그 인증서를 의존하고 있는 단체 모두가 신뢰하고 있는 제3의 단체다.
* 브라우저와 운영체제 : 모든 운영체제와 브라우저는 잘 알려진 인증기관의 목록을 보유하고 있다. 따라서 해당 소프트웨어 판매업체가 관리하고 있는 신뢰할 만한 단체의 목록을 자동적으로 신뢰하게 된다.
#### 인증서 폐기
* 인증서의 개인키가 노출되거나, 인증기관 자체가 해킹 공격을 당하는 경우, 혹은 인증서를 교체해야 하거나 소속 기관이 바뀌는 경우처럼 사요한 이유로도 인증서를 폐기할 수 있음
##### 인증서 폐기 리스트 (Certificate Revocation List, CRL)
* 각 인증기관은 폐기된 인증서의 시리얼 넘버를 리스트로 만들어 주기적으로 발표
##### 온라인 인증 상태 프로토콜(OSCP)
* 폐기된 인증서의 시리얼 넘버를 모두 보유하고 있는 CRL과는 달리, OCSP에서는 클라이언트가 직접 인증서 데이터베이스에 쿼리를 날려 시리얼 넘버의 유효성을 실시간으로 체크
#### TLS 레코드 프로토콜
* 여러 종류의 메시지(핸드셰이크, 알림, 혹은 'Content Type' 필드를 통한 데이터)를 구분하고, 각 메시지의 무결성을 체크하고 보호하는 기능
* 애플리케이션에 맞는 레코드 사이즈를 선택하는 것이 최적화에 중요한 영향을 미칠 수 있음
#### TLS 최적화
##### 연산처리 비용
* 암호화된 채널을 구축하고 관리하려면 양쪽 피어에게 추가 연산처리 비용이 듬
##### 조기종료
* 조기 종료를 구현하는 가장 간단한 방법은 데이터와 서비스를 전 세계에 골고루 분포된 서버에 복제하거나 캐시에 저장
##### 세션 캐싱과 상태를 유지하지 않는 재개
* TLS 세션 캐싱에 중요한 역할을 하는 세션 식별자는 SSL 2.0에서 처음으로 소개되었으며 대부분의 클라이언트와 서버도 이를 지원하고 있음
##### TLS 레코드 사이즈
* 레코드 사이즈가 작으면 오버헤드가 늘어나고, 레코드 사이즈가 크면 레이턴시가 늘어남
* 적당한 레코드 사이즈를 단순히 수치로 정의하기 어려움
##### TLS 압축
* 레코드 프로토콜 내에서 전송되는 데이터를 무손실로 압축하는 방법
* 2012년에 발표된 '범죄' 공격이 TLS 압축을 조정하여 비밀 인증 쿠키를 노출시킴으로써 해커들이 세션 하이재킹을 시도할 수 있다고 한다.
* 전송 계층 TLS 압축은 전송할 콘텐츠를 알 수 없기 때문에 이미 압축된 파일 (이미지, 비디오 등)을 재 압축할 우려가 있다.
* 이중 압축은 서버와 클라이언트 양쪽의 CPU 시간을 낭비시키므로 좋지 않음
##### 인증 사슬 길이
* 신뢰 사슬을 확인하기 위해서는 브라우저가 웹사이트의 인증서를 시작으로 상위 인증서 사슬을 하나씩 거쳐 신뢰할 수 있는 최상위까지 도달
##### OCSP 스테이플링(stapling)
* 모든 신규 TLS 커넥션에서는 브라우저가 인증서 사슬의 서명을 확인
* 브라우저는 해당 인증서가 폐기되었는지도 확인
* 브라우저는 주기적으로 인증기관의 CRL을 다운로드하여 캐시헤 저장, '실시간' 체크를 위해서 인증 단계에서 OCSP 요청을 보내기도 함
* 서버가 인증서 사슬에 OCSP 응답을 포함시킴으로써 브라우저가 온라인 체크 과정을 생략 할 수 있음 (OCSP 스테이플링)
##### HTTP 엄격한 전송 보안 (HTTP Strict Transport Security, HSTS)
* 서버가 HTTP 헤더를 이용해서 브라우저에게 접속 규칙을 지정하는 보안 정책 메커니즘
##### 성능 체크리스트
* TCP에서 얻을 수 있는 최고의 성능을 얻어라
* TLS 라이브러리를 최신 릴리스 버전으로 업그레이드하고 그에 맞추어 서버를 구축하라
* 세션 캐싱과 상태를 유지하지 않는 재개를 활성화하고 그에 맞게 설정하라
* 세션 캐싱 히트율를 모니터링하고 그에 맞게 설정을 조정하라
* TLS 세션을 사용자와 가까운 곳에서 종료시켜 왕복 레이턴시를 최소화하라
* TLS 레코드 크기를 단일 TCP 조각에 맞도록 조정하라.
* 인증 사슬이 초기 혼잡 윈도 크기를 넘지 않도록 하라.
* 인증 사슬에서 불필요한 인증서를 제거하라. 사슬의 길이를 최소화하라.
* 서버의 TLS 압축 기능을 비활성화하라
* 서버 이름 표시 (SNI)를 설정하라
* 서버의 OCSP 스테이플링 기능을 설정하라
* HSTS 헤더를 첨부하라
#### 테스트 검증
* Qualys SSL Server Test 온라인 서버를 이용
## 2부 무선 네트워크 성능
### 5장 무선 네트워크 소개
#### 유비쿼터스 환경
* 광범위하게는 무선 네트워크란 케이블 선으로 연결되지 않은 모든 네트워크를 말함
* WiFi, Bluetooth, ZigBee, NFC, WiMAX, LTE, HSPA, EV-DO, 초기 3G 표준, 위성서비스 등
##### 무선 네트워크의 종류
종류 | 범위 | 사용차 | 표쥰 규격
:--- | :--- | :--- | :---
Personal Area Network (PAN) | 개인 | 기기 간 무선 연결 | Bluetooth, ZigBae, NFC
Local Area Network (LAN) | 빌딩이나 구역 내 | 유선 네트워크의 무선 확장 | IEEE 802.11 (WiFi) 
Metropolitan Area Network (MAN) | 도시네트워크 간 | 무선 연결 | IEE 802.15 (WiMAX) 
Personal Area Network (PAN) | 전세계 | 무선 네트워크 연결 | Cellular (UMTS, LTE 등)
#### 무선 네트워크 성능의 기초 원리
##### 대역폭
* <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bae6b1970ad71932f5f9e41fcb283a72f78b3d7c">
* 네트워크 구간을 단독으로 사용하는 케이블로 연결하는 유선 네트워크와 달리 전파통신은 본질적으로 전파, 혹은 전자파라는 매체를 공유하게 됨
* 전체적인 채널의 비트 전송 속도는 지정된 주파수 범위와 정비례
* 모든 주파수 범위가 똑같은 성능을 보장해 주지는 않음
* 낮은 주파수의 신호는 더 멀리 넓게 이동하지만 커다란 안테나를 필요로 하고 해당 주파수 대역의 접속을 원하는 많은 클라이언트가 존재하여 경쟁이 치열
##### 신호 강도
* 신호 대 잡음 비(SNR), 혹은 송신자와 수신자 간의 신호 강도는 대역폭과 함께 무선통신의 근간이 되는 제약조건
* 모든 전파 통신은 이동 매체를 공유하기 때문에 다른 기기들이 방해 전파를 생성할 수 밖에 없음
* Near-far problem : 강한 신호가 약한 신호를 '밀어내' 수신자가 상대적으로 약한 신호를 받지 못하는 현상
* Cell-breathing : 누적된 잡음이나 방해 신호의 강도에 따라 통신 가능 구역, 혹은 신호 범위가 늘거나 주는 현상
##### 변조
* 디지털 신호를 아날로그 신호로 변환하는 과정이 바로 변조
* 몇 비트의 알파벳을 이용하느냐에 따라 변조 효율이 달라짐
* 알파벳과 심벌 레이트를 조합하여 통신 채널의 최종적인 처리량을 알아낼 수 있음
#### 실제 무선 통신 성능 측정하기
* 최대로 잡음과 방해 신호를 줄이고 송신자와 수신자를 가능한 가까이 두고 그들에게 필요한 전력을 모두 공급해 줘야 하고 또한 가장 효율적인 변조 방식을 선택해야 할 것
### 6장 와이파이 (WiFi)
* 와이파이 얼라이언스라는 무역 협회의 등록 상표로, 이 협회는 무선랜 기술들과 장치 간의 연결 표준 기술, 테스트 수행을 목적으로 설립
#### 이더넷에서 무선 랜으로
* 와이파이는 혼잡 회피 매커니즘에 의존
* 각 송신자가 충돌을 방지하기 위해 오직 채널이 유후 상태임을 확인했을 때 메시지 전체를 한꺼번에 보내도록 하는 방식
* 와이파이 프레임이 전송되면 전송자는 다음 ㅔㄷ이터를 전송하기 전에 수신자로부터 데이터를 받았다는 확실한 신호를 받을 때까지 대기
#### 와이파이 표준과 기능 들
802.11 protocol | Release | Freq(GHz) | Bandwidth(MHz) | Data rate per stream (Mbit/s)
:--- | :--- | :--- | :--- | :---
b | Sep 1999 | 2.4 | 20 | 1,2,5.5,11
g | Jun 2003 | 2.4 | 20 | 6,9,12,18,24,36,48,54
n | Oct 2009 | 2.4 | 20| 7.2, 14.4, 21.7, 28.9, 43.3, 57.8, 65, 72.2
n | Oct 2009 | 5| 40| 15, 30, 45, 60, 90, 120, 135, 150
ac| - 2014 | 5| 20,40,80,160| up to 866.7
#### 와이파이 성능의 측정과 최적화
* 전형적인 와이파이 성능이라는 것은 존재하지 않음
* 와이파이의 작동 범위는 사용하는 표준 기술, 사용자의 위치, 사용하는 기기, 그 지억에서 사용하는 무선전파 환경에 따라 달라짐
##### 와이파이 네트워크에서 패킷 손실
* 와이파이를 통해 데이터를 전달할 때에 TCP 패킷 손실은 분명 문제가 될 수 있으나, TCP 패킷 손실의 절대적인 비율을 놓고 보면 유선 네트워크와 크게 다를 바가 없음
#### 와이파이 네트워크의 최적화
##### 미계량 대역폭을 활용하라
#### 가변 대역폭에 적응하라
* 네트워크를 사용하는 위치가 조금이라도 바뀌거나 주변 무선 기기의 행동이 바뀌거나, 그 밖의 전반적인 무선 환경의 변화에 따라서 사용할 수 있는 대역폭은 시시작각으로 바뀜
##### 가변 레이턴시에 적응하라
### 7장 모바일 네트워크
#### 세대별 무선 네트워크 역사
세대 | 최고 데이터율 | 설명
:--- | :--- | :---
1G | 데이터 없음 | 아날로그 시스템
2G | Kbit/s | 아날로그 시스템과 병렬로, 혹은 중첩으로 구축된 첫 디지털 시스템
3G | Mbit/s | 아날로그 시스템과 병렬로 구축된 전용 디지털 네트워크
4G | Gbit/s | 디지털과 패킷만으로 운용되는 네트워크
##### 2G에서 시작된 데이터 서비스
##### 3GPP와 3GPP2 파트너십 
#### 3G 기술의 진화
##### 3GPP3 기술의 진화
##### IMT-Advanced 4G 요구사항
##### 롱텀 에볼루션 (Long Term Evolution, LTE)
##### HSPA+의 세계 4G 시장 선도
##### 다세대 네트워크를 위한 대비
#### 디바이스의 특징과 기능
##### 사용자 장비 카테고리
#### 무선 전파 리소스 컨트롤러
* 3G와 4G 네트워크에는 유선이나 와이파치 네트워크에 없는 무선 전파 리소스 컨트롤러(RRC)라는 독특한 기능이 있음
* RRC는 사용하는 기기와 기지국 사이의 모든 커넥션을 관리하고 중재
* RRC는 레이턴시, 처리량, 배터리 수명 등 사용 기기의 성능에 직접적으로 연관
* 엑세스 포인트에서 전송 트래픽 지시 메시지(delivery traffic indication message, DTIM)를 정기적으로 전송하여 특정 클라이언트에게 데이터 전송 사실을 알림
* RRC는 누가 언제 데이터를 보내고, 대역폭을 어떻게 할당하며 신호 전력을 어떻게 쓸 것이고 각 기기의 전력 상태는 어떤지 같은 수많은 변수를 관리
##### 3G, 4G 그리고 와이파이의 전력 요건
* 신호가 강하면 높은 처리량을 얻을 수 있다. 하지만 대량의 데이터 전송에는 상당한 에너지를 소비하기 때문에 배터리 수명에는 악영향
* 신호 강도를 낮추면 송신탑과의 연결 자체가 끊어질 수 있음
##### LTE RRC 상태 기계
* 모든 LTE 기기의 무선 전파 상태는 사용자와 연결된 방송탑에서 관리
* 3GPP 표준에서 정의한 RRC 상태 기계가 네트워크에 연결된 기기에게 가능한 모든 전력 상태를 정의
##### HSPA와 HSPA+(UMTS) RRC 상태 기계
##### EV-DO(CDMA) RRC 상태 기계
##### 비효율적인 주기적 전송
#### 통시사 네트워크 종단 간 아키텍처 (End-to-End Carrier Architecture)
##### 무선 접속망
##### 코어 네트워크
##### 백홀 용량과 레이턴시
#### 모바일 네트워크 패킷 흐름
##### 요청 보내기
##### 인바운드 데이터 흐름
#### 이종 네트워크
#### 실환경에서의 3G, 4G 그리고 WiFi성능
### 8장 모바일 네트워크 최적화
* 애플리케이션이 사용하는 프로토콜은 같아도 물리적인 전송 계층에 존재하는 여러 제약사항을 제대로 대처하지 않으면 응답속도가 느려지거나 레이턴시의 변동이 심해짐
* 네트워크 성능을 충분히 이해하지 않고 개발한 애플리케이션은 당연히 배터리 성능에도 좋지 않은 영향을 끼침
#### 배터리 전력 손실 줄이기
* 무선 전파가 활성화 되어 있을 때 데이터 전송을 최대화 시키고, 그 밖에 추가적인 데이터 전송을 줄이는 것을 목표로 함
#### 주기적이고 비효율적인 데이터 전송 제거
* 폴링(polling)은 모바일 네트워크에서는 특히 전력 소모가 많으므로 최소화하라.
* 가능하면 폴링 대시 푸시(push) 전달과 알림 신호가 사용되어야 한다.
* 아웃바운드와 인바운드 요청 모두 통합 처리해아 한다.
* 중요도가 낮은 요청은 무선 전파가 활성화될 때까지 미루어야 한다.
#### 네트워크 인터페이스의 종류를 다양하게 설계하기
#### 데이터를 한 번에 집중적으로 전송하고 유후 상태로 돌아가기
#### 와이파이(WiFi) 네트워크로 떠넘겨라
#### 프로토콜 애플리케이션 지침을 적용하라
## 3부 HTTP
### 9장 HTTP의 간략한 역사
#### HTTP 0.9:간단한 한 줄짜리 프로토콜
* 팀 버너스 리가 발표한 HTTP 초기 제안서에서는 간략함이라는 철학을 가지고 HTTP가 설계됨
* HTTP 0.9의 기능 요약
  * 클라이언트와 서버 간의 요청-응답 프로토콜이다.
  * TCP/IP 연결 위에서 동작하는 ASCII 프로토콜이다.
  * 하이퍼텍스트 문서(HTML)을 전송하기 위해 만들어졌다.
  * 서버와 클라이언트 간의 커넥션은 매번 요청이 끝날 때마다 종료된다.
#### HTTP 1.0:급속 성장과 정보성 RFC
* 1996년 5월에 이르러서는 HTTP 워킹 그룹에서 수많은 HTTP 1.0 버전의 공통적인 사용 시나리오를 담은 RFC-1945 발표
* RFC는 오로지 정보 제공 차원에서 만들어진 것으로써, 우리가 현재 알고 있는 HTTP 1.0은 공식 규격도 아니고 인터넷 표준도 아님
* HTTP 1.0의 주된 변환
  * 요청은 헤더 필드로 구분된 몇 개의 라인으로 구성될 수 있다.
  * 응답 오브젝트는 응답 상태 값을 앞부분에 표기하여 구성하고 있다.
  * 응답 오브젝트는 헤더 필드로 구분된 라인으로 구성되어 있다.
  * 응답 오브젝트는 하이퍼텍스트로 한정되어 있지 않다.
  * 서버와 클라이언트 간의 커넥션을 매번 요청이 끝날 때마다 종료된다.
#### HTTP 1.1:인터넷 표준
* HTTP 1.0의 문서화 작업과 더불어 HTTP를 공식적인 인터넷 국제 표준화 기구를 통해서 표준화하는 작업이 1995년에서 1999년에 걸쳐 동시에 진행
* HTTP 1.1 표준은 초기 버전 프로토콜의 애매모호한 부분들을 대폭 수정
* 킵얼라이브 커넥션, 블록 단위 인코딩 전송, 바이트 범위의 요청, 추가 캐시 메커니즘, 전송 인코딩, 요청 파이프라이닝 등의 기능을 추가하여 성능 최적화
#### HTTP 2.0: 전송 성능을 개선하다
* 2012년 초, HTTPbis 워킹 그룹이 HTTP 2.0에 대한 계획을 발표
* HTTP 2.0의 주된 목적은 전송 성능을 개선하고 낮은 레이턴시와 높은 처리량을 얻는 것
* HTTP 헤더, 수치, 사용 사례와 같은 상위 수준의 프로토콜 시맨틱은 전혀 번하지 않음
### 10장 웹 성능 이해의 첫걸음
#### 하이퍼텍스트, 웹 페이지 그리고 웹 애플리케이션
* 하이퍼텍스트 문서 : 하이퍼텍스트 문서는 월드 와이드 웹의 기원이며, 기본적인 서식과 하이퍼링크를 내재한 텍스트 문서
* 웹 페이지 : HTML 워킹그룹과 초기 브라우저 제작업체들이 하이퍼텍스트의 정의를 확장시켜 이미지나 오디오와 같은 하이퍼미디어 리소스를 제공할 수 있도록 했고 더 풍부한 레이아웃을 제공할 수 있는 토대를 마련
* 웹 애플리케이션 : 자바스크립트의 출현과 Dynamic HTML(DHTML)과 AJAX의 발전이 다시 한번 판도를 바꾸었고, 단순한 웹 페이지를 상호작용이 가능한 웹 애플리케이션으로 탈바꿈시킴
#### 현대 웹 애플리케이션의 구조
##### 속도, 성능 그리고 인간의 인지도
지연시간 | 사용자의 인식
:--- | :---
0 - 100ms | 즉각적
100 - 300ms | 인식할 만한 아주 작은 지연이 존재
300 - 1000ms | 기기가 정상적으로 동작하고 있다고 느낌
1,000+ms | 딴 생각을 하기 시작함
10,000+ms | 작업을 포기함
##### 리소스 워터폴 분석하기
* 리소스 워터폴은 사용 가능한 네트워크의 성능 진단 도구 중 가장 통찰력 있는 도구
#### 성능의 큰 기둥:컴퓨팅, 렌터링, 네트워킹
##### 대역폭을 늘려도 큰 영향은 없다
* 수십 개의 호스트로부터 수백 개의 작은 리소스를 얻어야 하는 일상적인 웹 브라우징에 있어서는 왕복 레이턴시가 더 큰 제한요소
##### 레이턴시가 성능에 병목이 된다.
* 대역폭을 업그레이드한다고 해서 웹 브라우징 속도가 그다지 빨라지지 않을 것이라는 결론
* 대용량 미디어 파일을 스트리밍하거나 업로드하는 속도는 빨라질지 몰라도 그 파일들을 담고 있는 페이지를 로딩하는 속도는 그다지 빨라지지 않음
##### 가상 사용자와 실제 사용자 성능 측정
* 웹 애플리케이션 성능을 측정하는 작업은 간단하지 않음
* 모든 애플리케이션에게 맞는 지표가 존재하지 않음
* 기준이 완성되면, 가상 사용자와 실제 사용자의 성능 측정을 결합한 성능 데이터를 수집
#### 브라우저 최적화
* 문서 인식 최적화 : 네트워킹 스택이 문서, CSS, 자바스크립트 파싱 파이프라인과 연계되어 있어서 중요한 네트워크 자산이 무엇인지 알아낸 후 우선순위를 정하여 그들을 먼저 전송한다. 그 후 가능한 한 빨리 페이지를 상호작용이 가능한
상태로 만든다. 이는 종종 리소스 우선순위 할당을 통해서나 룩어헤드 파싱, 혹은 이와 비슷한 기법을 통하여 수행된다.
* 추론 최적화 : 브라우저가 장시간 동안 사용자의 행동 패턴을 알아내어 추론 최적화를 수행 할 수 있다. 브라우저는 DNS 이름을 미리 알아내고 가능성 있는 호스트명에 미리 연결하는 등 사용자가 앞으로 행동을 예측할 수 있다.
* 리소스 프리페칭 및 우선순위 : 문서, CSS, 자바스크립트 파서가 네트워크 스택과 추가적인 정보를 나눔으로써 각 리소스의 상대적인 우선순위를 정할 수 있다. 첫 렌더링에 필요한 블로킹 리소스는 높은 순위를 부여받고, 낮은 순위의 요청은
임시적으로 대기열의 뒤로 이동하게 된다.
* DNS 사전 해결 : 연결 가능성이 높은 호스트명을 미리 알아내어 앞으로 있을 HTTP 요청에 DNS 레이턴시를 줄인다. 사전 해결은 사용자가 페이지의 링크 위에 마우스를 올리는 것 같은 네비게이션 기록을 통하여 학습한 것을 토대로 수행 할 
수 있다.
* TCP 사전 연결 : DNS 조회를 수행한 후에 브라우저는 HTTP 요청에 대비하여 TCP 커넥션을 예측하여 생성할 수 있다. 만약 예측이 맞았다면 TCP 핸드셰이크에 필요한 왕복 네트워크 레이턴시를 완전히 제거할 수 있게 된다.
* 페이지 사전 렌더링 : 일부 브라우저는 사용자가 앞으로 향할 목적지를 예측하여 그 페이지 전체를 숨겨진 탭에서 미리 렌더링해 놓을 수 있다. 그런 다음 사용자가 실제로 네비게이션을 시작할 때에 즉시 탭을 전환하게 된다.
### 11장 HTTP 1.x
* HTTP의 성능을 개선하는 것이 HTTP 1.1 워킹 그룹의 가장 핵싱적인 설계 목표
* 1.1 표준은 실제로 성능과 기능면에서 많은 개선을 이루어냄
* 성능과 기능 개선
  * 지속적으로 연결되어 있는 커넥션을 통해 재사용을 가능하게 함.
  * 청크된 전송 인코딩을 이용하여 응답 스트리밍을 가능하게 함.
  * 바이트 서빙 기법을 통하여 범위를 지정한 리소스 요청을 가능하게 함
  * 캐싱 메커니즘을 명확히 정의하고 성능을 향상시킴.
* DNS 룩업을 줄여라
  * 호스트명을 알아내는 데는 한 번의 네트워크 왕복이 필요
  * 룩업이 진행 중일 때는 요청을 받지 않으므로 요청 발생 시 레이턴시를 유발
* HTTP 요청을 최소화하라
  * 요청을 보내지 않아도 된다면 보내지 않는 것이 가장 좋은 최적화 방법
  * 페이지 내의 불필요한 리소스를 제거
* 콘텐츠 전송 네트워크를 이용하라 
  * 클라이언트와 지역적으로 가까운 곳에 데이터를 위치시키면 TCP 커넥션의 네트워크 레이턴시를 대폭 줄일 수 있고 처리량도 개선
* Expires 헤더를 추가하고 ETag를 설정하라
  * 중요한 리소스를 캐시에 저장하여 페이지의 모든 데이터마다 매번 요청을 보내지 않음
  * Expires 헤더는 오브젝트의 캐시 수명을 설정할 있음 
  * 사용자의 캐시에서 오브젝트를 직접 가져와 HTTP 요청 자체를 없앨 수 있음 
  * ETag와 Last-Modified 헤더는 마지막 업데이트된 시간을 기억함으로써 효율적인 캐시 재인증 매커니즘을 제공
* 데이터를 Gzip 압축하라
  * 텍스트 기반의 모든 데이터는 전송될 때에 Gzip으로 압축
  * Gzip은 서버에서 플래그를 설정하는 것만으로도 파일의 크기를 평균 60-80%까지 줄일 수 있음
  * 간단하면서도 큰 효과를 볼 수 있는 최적화 방법
* HTTP 리다이렉션을 피하라
  * HTTP 리다이렉션은 비용 소모가 큰 작업
  * 특히 클라이언트를 다른 호스트명으로 이동시켜야 할 때에는 DNS 룩업, TCP 커넥션 레이턴시 등의 추가적인 비용이 발생
#### 킵 얼라이브 커넥션의 장점
* 킵얼라이브 없이는 모든 요청이 두 번의 왕복 레이턴시를 유발
* 킵얼아이브 이용하면 첫 요청에서는 두 번의 왕복시간이 소요되지만, 그다음 요청부터는 단 한 번의 왕복 레이턴시가 소요
#### HTTP 파이프라이닝
* FIFO 큐를 클라이언트 쪽에서 서버 쪽으로 재배치하는 것
* 첫 요청이 끝나기를 기다리자 않고 바로 두 번째 요청을 처리하기 시작
* 서버상에 병렬적으로 처리는 미묘한 문제를 일으키며 HTTP 1.x 프로토콜에서는 중요한 한계점을 드러냄
* HTTP 1.x은 같은 커넥션에서 여러 응답을 섞어 보내지 않고, 응답의 모든 바이트가 완전히 전송되기 전에는 다음 응답을 전송할 수 없음
* HTTP 파이프라인을 도입하는 작업은 많은 이점이 있음에도 불구하고 여전히 매우 한정적으로 이루어짐
#### 다수의 TCP 커넥션을 사용하기
* 호스트당 여섯 개의 독립적인 커넥션
  * 장점
    * 클라이언트는 병렬로 최대 여섯 개의 요청을 보낼 수 있음
    * 서버는 병렬로 최대 여섯 개의 요청을 처리 할 수 있음
    * 첫 왕복에 전송 가능한 누적 패킷 수는 여섯 배로 늘어남
  * 단점
    * 소켓 수가 늘어나면서 클라이언트, 서버, 그리고 모든 중계자가 추가 메모리 버퍼와 CPU 오버헤드 등 리소스를 소모
    * 병렬 TCP 스트림 간에 대역폭 경쟁이 일어남
    * 여러 소켓을 동시에 처리하는 작업을 구현하는데 까다로움
    * 병렬의 TCP 스트림을 활용해도 애플리케이션에서는 병렬 작업이 제한적으로 이루어짐
  * 채택 이유
    * 애플리케이션 프로토콜(HTTP)이 갖는 제약의 해법
    * TCP의 혼잡 윈도 크기의 낮은 시작값 문제의 해법
    * TCP 윈도 스케일링을 사용할 수 없는 클라이언트를 위한 해법
#### 도메인 샤딩
* 모든 리소스를 하나의 호스트에서 제공하는 대신 수동으로 그들을 하나의 호스트에서 제공하는 대신 수동으로 그들을 하위 도메인으로 쪼갤 수 있음
* 호스트명마다 추가적으로 DNS 룩업이 필요하고 소켓이 추가될 때마다 클라이언트와 서버 양쪽의 리소스를 소모
* 리소스가 어디로 어떻게 쪼개지는가를 사이트 관리자가 직접 관리
* 주의사항
  * TCP를 최대한 활용
  * 브라우저는 자동으로 여섯 개의 커넥션을 생성
  * 리소스의 수, 크기, 응답 시간에 따라 최적의 샤딩 수가 달라짐
  * 클라이언트 레이턴시와 대역폭에 따라 최적의 샤딩 수가 달라짐
  * 도메인 샤딩은 추가적인 DNS 룩업과 TCP 느린 시작으로 인하여 성능에 영향을 줄 수 있음
#### 프로토콜 오버헤드를 측정하고 조절하기
* 매우 반복적이고 압축되지 않은 헤더 데이터를 줄일 수 있다면, 모든 네트워크 왕복 레이턴시를 절약하야 많은 웹 애플리케이션의 성능을 향상시킬 수 있을 것
#### 결합과 스프라이팅
* 결합 : 여러 개의 자바스크립트나 CSS 파일을 하나의 리소스로 결합하는 것
* 스프라이팅 : 여러 이미지를 하나의 거대 이미지로 붙이는 것
* 이점
  * 프로토콜 오버헤드 감소
    * 파일들을 하나의 리소스로 결합시킴으로써 각 파일마다 발생하는 프로토콜 오버헤드를 없앨 수 있음
  * 애플리케이션 계층 파이프라이닝
    * 바이트 전송만 놓고 봤을 때에는 위 두 기법이 주는 결과는 HTTP 파이프라이닝을 사용한 것과 같음
* 단점
  * 추가 전처리이나 운영상의 코드 배치, 코드 면에서 복잡성을 더할 가능성이 있음
  * 독립적인 리소스 여러 개를 한데 묶는 것은 캐시 성능과 페이지의 실행 속도에 좋지 않은 영향을 미칠 수 있음
  * 결합된 묶음은 현재 페이지에서 필요하지 않은 리소스를 포함할 수 있음
  * 낱개 파일에 업데이트를 적용하려면 묶여 있는 번들 전체를 다운로드해야만 하므로 오버헤드 비용 발생
  * 자바스크립트와 CSS 모두 전송이 완료되고 나서야 파싱 되어 실행되므로, 애플리케이션의 실행 속도를 지연시킬 가능성 있음
#### 리소스 인라이닝
* 문서 자체에 리소스를 삽입함으로써 요청 수를 줄임
* 실제 환경에서는 리소스 인라이닝은 1-2KB 이하의 파일에 적용하도록 권장
* 삽입된 리소스가 빈번하게 변경된다면 호스트에 존재하는 문서의 캐시를 불필요하게 자주 무효화 시킴
### 12장 HTTP 2.0
#### 스피디(SPDY)의 역사
* 스피디는 2009년도 중반에 구글에서 발표한 실험적인 프로토콜
#### HTTP 2.0 으로 가는 길
* 스피디는 HTTP 2.0의 개발을 자극하는 기폭제 역할을 하였지만 스피디가 HTTP 2.0인 것은 아님
* 핵심 디자인
  * TCP를 사용하여, 대부분의 경우 HTTP 1.1보다 대폭적으로 사용자단의 레이턴시를 개선해야 함
  * HTTP의 문제점인 'HOL 블로킹'을 해결해야 함
  * 병렬화를 위하여 서버에 다수의 커넥션을 요구하지 않고, 특히 혼잡 제어에 있어서 TCP 사용 효율을 높여야 함
  * 기존의 스펙 문서를 활용하여 HTTP 메서드, 상태 코드, URI, 헤더 필드를 비롯한 HTTP 1.1의 기본 틀은 유지해야 함
  * HTTP 2.0이 HTTP 1.x와 어떻게 상호작용을 하는지 명확하게 정의해야 함
  * 새롭게 확장되는 영역을 명확하게 구분 짓고 각각의 적절한 사용방법에 대한 정책을 마련해야 함
* HTTP 2.0은 이전 표준에서 잘 알려진 성능적인 한계점을 극복하는 동시에 이전 1.x 표준을 확장하고자 하는 것
* HTTP의 애플리케이션 시맨틱이나 HTTP 메서드, 상태 코드, URI, 헤더 필드 기존 기능과 핵심 콘셉트에는 바뀌는 점 없음
#### 디자인과 기술적인 목표
##### 바이너리 프레이밍 계층
* HTTP 2.0의 성능 개선의 핵심은 새로 도입된 바이너리 프레이밍 계층
* '계층'은 소켓 인터페이스와 애플리케이션에게 노출되어 있는 상위 계층 HTTP API 사이를 잇는 새 메커니즘
* 개행(newline)으로 구분되던 단순 텍스트 HTTP 1.x 프로토콜과는 달리, HTTP 2.0 커뮤니케이션은 모두 바이너리 형식의 더 작은 메시지와 프레임으로 나뉨
* 클라이언트와 서버 모두 바이너리 인코딩 메커니즘을 사용해야 함
##### 스트림, 메시지 그리고 프레임
* 스트림
  * 생성된 커넥션 내에서 발생하는 데이터의 양방향 흐름
  * 커넥션 내에 존재하는 가상 채널로서 양뱡향으로 메시지를 전달한다. 각 스트림은 고유의 정수 식별자를 가짐
* 메시지
  * 논리적 메시지를 이루는 전체 프레임 시퀸스
  * 요청이나 응답과 같은 논리적 HTTP 메시지이며 하나 이상의 프레임을 포함
* 프레임
  * HTTP 2.0 커뮤니케이션에서 사용되는 가장 작은 단위으로, 각 프레임이 어느 스트림에 속해 있는지 지정하는 프레임 헤더를 포함
  * 가장 작은 커뮤니케이션 단위으로, HTTP 헤더, 페이로드 등의 특정 형식의 데이터를 운반
##### 요청과 응답 멀티플렉싱
* HTTP 2.0에서 도입된 바이너리 프레이밍 계층은 HTTP 메시지를 톡립적인 프레임으로 쪼개어 전달한 후 수신 측에서 재구성하기 때문에 완전한 요청과 응답 멀티플렉싱이 가능
* HTTP 메시지를 독립적인 프레임으로 분할하고 교차 삽입하여 수신 측에서 재구성할 수 있도록 한 것
* 개선사항
  * 다수의 요청을 병렬로 교차 삽입하여 어느 하나의 요청도 블로킹하지 않음
  * 다수의 응답을 병렬로 교차 삽입하여 어느 하나의 응답도 블로킹하지 않음
  * 다수의 요청과 응답을 하나의 커넥션을 통해 병렬로 전달
  * 불필요한 레이턴시를 없애 페이지 로딩 시간을 줄임
  * 애플리케이션 코드에 불필요하게 추가된 HTTP 1.x 요소를 없앨 수 있음
##### 요청 우선순위
* HTTP 메시지를 여러 프레임으로 나누고 나면, 각 프레임의 배치 순서나 전달 순서를 최적화하여 애플리케이션의 성능을 한층 더 개선 할 수 있음
* 0은 가장 높은 순위의 스트림 2의 31승 -1은 가장 낮은 순위의 스트림을 나타냄
* HTTP 2.0은 우선순위를 정할 때에 특별한 알고리즘을 정하지는 않고, 그저 클라이언트와 서버 간 우선순위에 따라 데이터 교환이 이루어지는 메커니즘만을 제공
##### 요청 지점당 하나의 커넥션
* 요청 지점당 커넥션 수를 하나로 제한하면 그에 따른 오버헤드를 대폭 줄일 수 있음
* 커넥션 경로를 관리할 소켓의 수가 줄고, 메모리 소모가 감소하기 때문에 커넥션의 처리량도 증가
* 모든 스트림 간에 일관적인 우선순위
* 단일 압축 컨텍스트를 통한 향상된 압축률
* TCP 커넥션을 줄임으로써 네트워크 혼잡을 개선
* 느린 시작 시간 단축과 빠른 혼잡 및 손실 복구
##### 흐름 제어
* 동일한 TCP 커넥션상에서 여러 스트림을 멀티플렉싱하면 공유 대역폭 리소스를 두고 경쟁이 일어남
* 스트림의 우선순위 정보로 상대적인 전달 순서를 결정할 수는 있지만 오직 우선순위 정보로만 가지고는 스트림과 여러 커넥션 사이에서 적절하게 리소스를 할당하기 어려움
* HTTP 2.0에서는 스트림과 커넥션 흐름 제어에 대해 간단한 매커니즘을 제공
* 흐름 제어는 종단이 아닌 홉 간 에서 작동
##### 서버 푸시
* HTTP 2.0의 막강한 기능 중 하나는 바로 서버가 클라이언트 요청 하나에 대하여 여러 개의 응답을 보낼 수 있는 기능
###### HTTP 2.0 서버 푸시 구현하기
* 서버 푸시는 애플리케이션이 최적의 방식으로 데이터를 운반할 수 있도록 여러가지 기회를 가져다줌
##### 헤더 압축
* HTTP 2.0은 헤더 메타데이터를 압축함
* 각 요청과 응답마다 같은 데이터를 재전송하는 대신에, HTTP 2.0은 클라이언트와 서버 양측에서 이전에 보낸 key-value 쌍을 저장한 '헤더 테이블'을 사용
* 헤더 테이블은 HTTP 2.0 커넥션이 살아 있는 동안 계속 존재하며, 클라이언트와 서버 양쪽에서 점진적으로 업데이트 됨
* 각 신규 헤더 key-value 쌍은 기존 테이블에 추가되거나 테이블 안에 있는 이전 값을 대체
* 결과적으로 HTTP 2.0 커넥션의 양쪽에서는 어느 헤더가 이미 전송되었고, 이전 값이 무엇이었는지 알 수 있음
* 새 헤더의 묶음은 이전 묶음과 달라진 내용만으로 구성될 수 있음
##### 효율적인 HTTP 2.0 업그레이드와 새로운 발견
#### 바이너리 프레이밍의 간략한 소개
* 모든 프레임은 공통적으로 8바이트 헤더를 포함하고 있으며, 이 헤더 안에는 프레임의 길이, 형식, 플래그용 비트 필드, 그리고 31비트 스트림
##### 새 스트림 생성하기
* 클라이언트가 HEADERS 프레임을 보내 새 요청을 생성
* 새 스트림의 식별자(ID), 선택적 31비트 우선순위 값, 그리고 페이로드 내에서 HTTP 헤더 key-value 쌍을 담은 공통 헤더
* 서버는 푸시 스트림을 생성하여 PUSH_PROMISE 프레임을 보냄
* HEADERS 프레임과 거의 동일하지만, 우선순위 값 대신 '약속한 스트림 식별자(ID)'를 전달
##### 애플리케이션 테이터 전송하기
* 새 스트림을 생성하고 HTTP 헤더를 전송하고 나면, 애플리케이션 데이터가 존재할 경우 DATA 프레임을 통하여 애플리케이션 페이로드를 전송
* DATA 프레임에 걸쳐 나뉠 수 있고, 마지막 프레임에는 메시지의 끝을 나타내는 END_STEAM 플래그를 프레임의 헤더 내에 포함
* 페이로드에는 추가적인 인코딩이나 압축을 수행 하지 않음
##### HTTP 2.0 프레임 데이터 흐름 분석하기
### 13장 애플리케이션 전송 최적화
#### 성능 지침 표본
* DNS 룩업을 줄여라
* TCP 커넥션을 재사용하라
* HTTP 리다이렉션 수를 최소화하라
* CDN(Content Delivery Network)을 이용하라
* 불필요한 리소스를 제거하라
* 클라이언트에서 리소스를 캐시하라
* 불필요한 요청 데이터 자체를 없애라
* 요청과 응답을 병렬로 처리하라
* 프로토콜 고유의 최적화를 적용하라
##### 클라이언트에서 리소스를 캐시하라
##### 전송된 데이터를 압축하라
* HTML, CSS, 자바스크립트와 같은 텍스트 기반의 리소스는 Gzip으로 압축했을 경우, 그 크키가 평균 60~80까지 줄어듬
* 알맞은 이미지 형식을 선택한 후에는 이미지 파일의 크기가 불필요하게 크지 않도록 리사이징
##### 불필요한 요청 자체를 없애라
##### 요청과 응답 처리를 병렬화하라
#### HTTP 1.x 최적화
* HTTP 파이프라이닝을 활용하라
* 도메인 샤딩을 적용하라
* 리소스를 하나로 묶어 HTTP 요청 수를 줄여라
#### HTTP 2.0 최적화
##### 1.x 최적화 없애기
* 한 요청 지점당 하나의 커넥션을 사용하라
* 불필요한 파일 결합과 이미지 스프라이팅을 없애라
* 서버 푸시를 활용하라
##### 듀얼 프로토콜 애플리케이션 전략
* 동일한 애플리케이션 코드와 듀얼 프로토콜 사용
* 별개의 애플리케이션 코드와 듀얼 프로토콜 사용
* HTTP 1.x와 HTTP 2.0의 유동적인 최적화
* HTTP 2.0 단일 프로토콜 사용
##### 1.x에서 2.0으로, 그리고 다시 1.x으로
##### 서버 품질과 성능을 평가하기
##### TLS가 있을 때와 없을 때 2.0을 사용하기
##### 로드 밸런서, 프락시 그리고 애플리케이션 서버
## 4부 브라우저 API와 프로토콜
### 14장 브라우저 네트워킹의 첫걸음
#### 커넥션 관리와 최적화
* 출처(Origin) : 애플리케이션 프로토콜, 도메인명, 그리고 포트 수로 이우러진 그룹
* 소켓 풀 : 같은 출처에 속한 소켓의 그룹
* 자동 소켓 풀링은 TCP 커넥션 재사용을 자동화하여 성능 개선에 큰 도움을 줌
  * 브라우저는 대기 중인 요청을 우선순위의 순서대로 처리 할 수있음
  * 브라우저는 레이턴시를 최소화하고 처리량을 개선하기 위하여 소켓을 재사용 할 수 있음
  * 브라우저는 요청을 예상하여 능동적으로 소켓을 열 수 있음
  * 브라우저는 모든 소켓에 걸쳐 대역폭 할당을 최적화할 수 있음
#### 네트워크 보안과 샌드박싱
* 커넥션 수 제한 : 브라우저는 열려 있는 모든 소켓 풀을 관리하여 커넥션 수를 제한
* 요청 서식 포맷과 응답 프로세싱 : 브라우저는 서버를 보호하기 위해 전송되는 요청 전체를 포맷하여 지속적이고 잘 구성된 프로토콜 시맨틱을 구성
* TLS 협상 : 브라우저는 TLS 핸드셰이크를 통해 필요한 인증서 확인 작업을 수행
* 동일 출처 정책 (Same-origin policy) : 브라우저는 어떤 요청이 애플리케이션을 통해 어느 출처에서 시작된 것인가에 대해 제한을 둠
#### 리소스와 클라이언트 상태 캐싱
* 리소스 캐시를 확인하여 필요한 확인 절차를 거침
* 정해진 요건을 충분히 갖추었다면 리소스의 로컬 복사본을 전송
* 캐시에 로컬 리소스가 존재하지 않으면 네트워크 요청이 생성
* 접속이 허용되면 그에 대한 응답을 자동으로 캐시에 저장
#### 애플리케이션 API 프로토콜
 / | XMLHttpRequest | Server-Send Events  | WebSockets
:--- | :--- | :--- | :---
요청 스트리밍 | no | no | yes
응답 스트리밍 | limited | yes | yes
프레이밍 메커니즘 | HTTP | event stream | binary framing
바이너리 데이터 전송 | yes | no(base64) | yes
압축 | yes | yes | limited
애플리케이션 전송 프로토콜 | HTTP | HTTP| WebSocket
네트워크 전송 프로토콜 | TCP | TCO | TCP
### 15장 XMLHttpRequest
* XMLHttpRequest(XHR)는 클라이언트가 자바스크립트 언어를 통해 스크립트 데이터를 전송할 수 있도록 해주는 브라우저 레벨의 API
#### XHR의 간략한 역사
* XML의 접두어는 XHR이 인터넷 익스플로러 5 버전에 MSXML 라이브러리의 한 부분으로 처음 출시했던 흔적
* 모질라 브라우저는 마이크로소프트에 대응하여 그들만의 XHR을 구현하였고, XMLHttpRequest 인터페이스를 통해 공개
* 사파리, 오페라 등 다른 브라우저 또한 이 인터페이스를 준수
* 초기 버전의 XHR은 텍스트만 허용하는 데이터 전송, 업로드 기능에 제한된 지원, 크로스-도메인 요청을 처리하지 못함
* 단점들을 극복하기 위해 XMLHttpRequest Level 2 초안이 다음과 같은 새로운 기능들을 추가 2008년 발표
* 2011년에 XMLHttpRequest Level 2 스펙이 원래 XMLHttpRequest 초안문서에 통합
#### 교차 출처 자원 공유 (Cross-Origin Resource Sharing, CORS)
* CORS는 클라이언트 측의 cross-origin 요청에 대한 안전한 사전동의(Opt-In) 메너키즘을 제공
#### XHR로 데이터 다운로드하기
#### XHR로 데이터 업로드하기
#### 다운로드와 업로드 모니터링 하기
#### XHR로 데이터 스트리밍 하기
#### 실시간 알림과 전달
##### XHR 폴링
* 폴링은 폴링 주기가 긴 애플리케이션, 그리고 새로운 이벤트가 예측 가능한 시간에 오는 경우, 마지막으로 전송되는 페이로드가 큰 경우에 적합
##### XHR 롱폴링
* 서버에 업데이트가 발생할 때까지 연결을 계속 유지함으로써 데이터는 클라이언트로 즉각 전송
#### XHR 사용 사례와 성능
* 사용 사례
  * 브라우저 안에서 비동기 통신을 가능케 함
  * 프로셋를 단순하게 만들어 주기도 함
* XHR은 실시간 전달을 위한 메커니즘이지만 최상의 성능을 보장하지 않음
* Server-Sent Event, WebSocket 같은 방법을 지원
### 16장 서버 발송 이벤트
* 서버 발송 이벤트(Server-Sent Events, SSE)는 실시간 알림이나 서버에서 발생하는 업데이트 같은 텍스트 기반 이벤트 데이터를 서버에서 클라이언트로 스트리밍 해줌
#### EventSource API
#### 이벤트 스트림 프로토콜
#### SSE 사용 사례와 성능
### 17장 웹소켓
* 웹소켓은 클라이언트와 서버 사이에 양방향의 텍스트 및 바이너리 데이터 스트리밍을 가능케 함
* 웹소켓은 브라우저에서 가장 유연하고 다목적인 전송 방식
#### 웹소켓 API
```javascript
var ws = new WebSocket('wss://example.com/socket');

ws.onerror = function (error) { };

ws.onclose = function() { }

ws.onopen = function () { 
    ws.send("Connection established. Hello server!");
}

ws.onmessage = function (msg) { 
    if(msg.data instanceof Blob) {
        processBlob(msg.data);
    } else {
        processText(msg.data);
    }
  }
```
#### WS와 WSS URL 스킴
* 웹소켓 리소스 URL은 자신만의 커스텀 스킴을 사용
* 단순 텍스트 커뮤니케이션에서는 ws을, 암호화된 채널에서는 (TCP + TLS) wss가 필요
* 웹소켓 wire 프로토콜은 브라우저 바깥에서도 사용이 가능하고 HTTP를 사용하지 않고도 협상이 가능 따라서 HyBi 워킹 그룹에서는 커스텀 URL 스킴을 채택
#### 텍스트와 바이너리 데이터를 수신하기
* 브라우저에서 새 메시지가 도착하면, 메시지가 텍스트인 경우 자동적으로 DOMString 오브젝트로 변환
* 바이너리 데이터인 경우 Blob 오브젝트로 변환
```javascript
var ws = new WebSocket('wss://example.com/socket');
ws.binaryType = "arraybuffer";

ws.onmessage = function (msg) { 
    if(msg.data instanceof ArrayBuffer) {
        processArrayBuffer(msg.data);
    } else {
        processText(msg.data);
    }
  }
```
#### 텍스트와 바이너리 데이터를 전송하기
```javascript
var ws = new WebSocket('wss://example.com/socket');

ws.onopen = function () { 
    socket.send("Hello server!");
    socket.send(JSON.stringify({'msg':'payload'}));
    
    var buffer = new ArrayBuffer(128);
    socket.send(buffer);
    
    var intview = new Uint32Array(buffer);
    socket.send(intview);
    
    var blob = new Blob(buffer);
    socket.send(blob);
 }
```
* send()는 비동기 메소드
* 제공된 데이터가 클라이언트의 대기열에 들어가면 함수는 그 즉시 리턴
##### 서브프로토콜 협상
* 웹소켓 프로토콜은 메세지의 형식을 예측하지 않고, 오직 1비트 크기의 플래그를 이용하여 메시지의 내용이 텍스트인지 바이너리 데이터인지를 구분
* HTTP, XHR 요청 처럼 매 요청과 응답에 HTTP 헤더를 통해 추가적인 메타데이터를 전송할 필요가 없음
* 클라이언트와 서버가 이 데이터를 주고받기 위해서 양측의 동의에 따라 자신들만의 서브프로토콜을 사용
#### 웹소켓 프로토콜
##### 바이너리 프레이밍 계층
* 웹소켓은 커스텀 바이너리 프레이밍 형식을 사용
* 모든 애플리케이션 메시지를 하나 이상의 프레임으로 쪼갠 후, 목적지로 전송하여 메시지를 재구성한 뒤 메시지 전체가 수신자에게 도착하면 알림을 보냄
* 프레임 : 최소 커뮤니케이션 단위. 각 프레임은 가변 길이의 프레임 헤더와 애플리케이션의 전체 혹은 일부분을 담고 있는 페이로드로 이루어짐
* 메시지 : 논리적인 애프리케이션 메시지를 이루는 전체 시퀀스
##### 프로토콜 확장 기능
* 웹소켓은 프로토콜 확장이 가능
* 웹소켓 프로토콜의 통신 형식과 시맨틱을 확장해 새로운 연산 코드와 데이터 필드를 정의 할 수 있음
* 웹소켓 기술 명세에서는 전송되는 데이터를 압축하기 위한 메커니즘이나 규정 따위는 존재하지 않음 
##### HTTP 업그레이드 협상
#### 웹소켓 사용 사례와 성능
##### 요청과 응답 스트리밍
* 웹소켓은 양방향 모두에서 텍스트와 바이너리 애플리케이션 데이터를 낮은 레이턴시로 운반할수 있음
##### 메시지 오버헤드
* 애플리케이션 메시지는 하나 이상의 프레임으로 쪼개지고, 각 프레임마다 2-14의 오버헤드가 발생
##### 데이터 효율과 압축
##### 커스텀 애플리케이션 프로토콜
* 스트리밍을 이용하면 클라이언트와 서버 사이에 커스텀 프로토콜을 전달할 수 있음
##### 웹소켓 인프라 활용하기
#### 성능 체크리스트
* 안정적인 사용을 위해 보안 웹소켓(TLS를 통한 WSS)을 이용하라
* Polyfill 성능을 주시하라
* 서브프로토콜 협상을 활용하여 애플리케이션 프로토콜을 지정하라
* 바이너리 페이로드를 최적화하여 전송 데이터 크기를 최소화하라
* UTF-8 콘텐츠를 압축하여 전송 데이터 크기를 최소화하라
* 수신된 바이너리 페이로드에게 알맞은 바이너리 형식을 지정한다.
* 클라이언트 쪽의 버퍼에 저장된 데이터의 양을 관찰하라
* 커다란 애플리케이션 메시지를 잘게 쪼개어 헤드-오브-라인 블로킹을 피하라
* 다른 전송 방식을 적절하게 활용하라
### 18장 WebRTC
* 웹 실시간 커뮤니케이션은 브라우저끼리 피어-투-피어 오디오, 비디오,데이터 공유를 가능케 하는 표준, 프로토콜, 자바스크립트 API를 통틀어 부르는 말